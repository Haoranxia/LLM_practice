{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8491ea9c",
   "metadata": {},
   "source": [
    "# Distributed Representation for language modelling\n",
    "\n",
    "- Seems to be quite similar to how modern LLMs work but with an MLP instead of Transformer. \n",
    "\\\n",
    "The only difference is that modern LLMs use more modern techniques (Transformers, AdamW optimizer, adaptive learning rate, way more parameters, post-training, etc...) \n",
    "\\\n",
    "Same fundemantel ideas. Different architecture and scale.\n",
    "- Words, Letters, or more generally tokens are represented as feature vectors. \n",
    "- Embedding of words is trained by backprop: \\\n",
    "Words -> Embedding -> Predict next token (using a n-gram context window)\n",
    "- We train using a dataset of n-gram \"sentences\". \n",
    "\n",
    "#### General setup:\n",
    "- **Embedding layer**: \\\n",
    "Map vocab token to an embedding via a lookup table (i.e. a matrix multiplication on one-hot encoded input token). Do this for each token in the context window.\n",
    "\n",
    "- **MLP layer + nonlinearity**: \\\n",
    "Pass input to MLP and output logits for predicting the next token. The input size is the size of our context window and embedding dimensionality, outputs some feature vector of custom dimensionality.\n",
    "\n",
    "- **Mapping feature to logits**: \\\n",
    "We then map the feature vector (hidden layer vector) h to a \"score\" over the vocab:\\\n",
    "$logit = b + Wh + Ux \\in \\mathbb{R}^{|V|}$\\\n",
    "where $Wh$ is the mapping from feature to vocab. and $Ux$ represents a \"skip layer\" of some sorts coming all the way from the input\n",
    "\n",
    "- **Softmax**: \\\n",
    "Compute the softmax over logits to obtain probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb630c",
   "metadata": {},
   "source": [
    "#### Setup:\n",
    "\n",
    "1. Onehot encode N inputs (context window size) into a matrix of (N, 27)\n",
    "2. \"extract\" embedding from lookup table using onehot-encoding vector via a matrix multiplication \\\n",
    "Since onehot @ emebdding matrix = extraction of a specific row (which represents the embedding) \\\n",
    "Onehot (N, 27) @ Embedding (27, $D_{emb}$) = Embedding matrix (N, $D_{emb}$)\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "08e25cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d2575202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset of samples with (N-1) context \n",
    "words = open('./datasets/names.txt', 'r').read().splitlines()\n",
    "print(min(len(w) for w in words))  # shortest name length\n",
    "print(max(len(w) for w in words))  # longest name length\n",
    "\n",
    "# NOTE: Need to handle edge cases where names are shorter than context length\n",
    "\n",
    "# Compute the statistics of the whole dataset in a dictionary\n",
    "counts = {}\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']      \n",
    "    for a, b in zip(chs, chs[1:]):             # iterate over all bigram pairs\n",
    "        if a not in counts:\n",
    "            counts[a] = {}                     # create a new entry if a hasn't been seen before\n",
    "\n",
    "        if b not in counts[a]:                 # create a new entry if b_ hasn't been seen before\n",
    "            counts[a][b] = 0\n",
    "        counts[a][b] += 1                      # increment count for the pairs we encounter\n",
    "\n",
    "# Create character-level vocabulary\n",
    "charset = sorted(list(counts.keys()))               # get all the unique characters  \n",
    "stoi = {s: i for i, s in enumerate(charset)}        # string to index\n",
    "itos = {i: s for s, i in stoi.items()}              # index to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2d1fe2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3])\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_WINDOW = 3\n",
    "VOCAB_SIZE = 27         # 26 letters + 1 for special 'end of name' character\n",
    "\n",
    "# Convert dataset into input/output samples for training\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    context = [0] * CONTEXT_WINDOW          # initialize context with all '.'\n",
    "    for ch in w + '.':                      # for each character in the name + end character\n",
    "        ix = stoi[ch]                       # get the index of the character\n",
    "        X.append(context)                   # add the current context to inputs\n",
    "        Y.append(ix)                        # add the current character index to outputs\n",
    "        context = context[1:] + [ix]        # update context by appending the current character and removing the starting char\n",
    "\n",
    "X = th.tensor(X)\n",
    "Y = th.tensor(Y)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b9c551a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 2])\n",
      "torch.Size([228146, 3])\n",
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        ...,\n",
      "        [26, 26, 25],\n",
      "        [26, 25, 26],\n",
      "        [25, 26, 24]])\n",
      "torch.Size([228146, 3, 2])\n",
      "tensor(0)\n",
      "tensor(-0.2465)\n"
     ]
    }
   ],
   "source": [
    "# Scratchpad/experimenting area\n",
    "E = th.randn(27, 2)\n",
    "print(E.shape)\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "# what happens is that X contains the indices into E has values. Only contains values from 0 to 27\n",
    "# When we do E[X], it retrieves the rows of E corresponding to the indices in X which has 2 elements\n",
    "# Since there are 3 elements per row in X, we get [32, 3, 2] = [nr_data, context_window, emb_dim]. \n",
    "# In other words, for each of the 32 samples, we have 3 context characters, each represented by a 2-dimensional embedding vector.\n",
    "# Indexing using tensors or lists returns a new tensor, hence why this is possible\n",
    "print(E[X].shape)   \n",
    "print(X[0, 0])\n",
    "print(E[0, 0])\n",
    "\n",
    "# E = Embedding Matrix (|V|, emb_dim)\n",
    "# X = input data (nr_data, context_window)\n",
    "\n",
    "# one_hot = F.one_hot(X[:5], num_classes=VOCAB_SIZE).float()   # shape: (nr_data, context_window, vocab_size)\n",
    "# print(one_hot.shape)\n",
    "# manual_emb = one_hot @ E                                     # shape: (nr_data, context_window, emb_dim)\n",
    "# print(manual_emb.shape)\n",
    "\n",
    "# emb = E[X[:5]]                                               # shape: (nr_data, context_window, emb_dim)\n",
    "# print(E[X[:5]].shape)\n",
    "\n",
    "# print(manual_emb == emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "263a7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the dataset \n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "# Train split\n",
    "train_end = int(0.8 * len(X))\n",
    "train_dataloader = DataLoader(NameDataset(X[:train_end], Y[:train_end]), batch_size=32, shuffle=True)\n",
    "\n",
    "# Dev split\n",
    "dev_end = int(0.9 * len(X))\n",
    "dev_dataloader = DataLoader(NameDataset(X[train_end:dev_end], Y[train_end:dev_end]), batch_size=32)\n",
    "\n",
    "# Test split\n",
    "test_dataloader = DataLoader(NameDataset(X[dev_end:], Y[dev_end:]), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24841e5",
   "metadata": {},
   "source": [
    "# Manual Tensor NN\n",
    "First model uses only tensors and little pytorch ready functionality.\n",
    "\n",
    "Model parameter dimensions: \n",
    "1. $ y = b + U tanh(d + Hx) $ \n",
    "2. $ y \\in \\mathbb{R}^{|V|} $ \n",
    "3. $ b \\in \\mathbb{R}^{|V|}$ \n",
    "4. $ U \\in \\mathbb{R}^{|V| , h}$ \n",
    "5. $ d \\in \\mathbb{R}^{h}$ \n",
    "6. $ H \\in \\mathbb{R}^{h , (n-1) m}$ \n",
    "7. $ x \\in \\mathbb{R}^{(n-1)m}$ \n",
    "\n",
    "\n",
    "#### How to determine a good learning rate?\n",
    "1. Find a good learning rate from a range of learning rates. An interval of exponentially increasing learning rates.\n",
    "2. Schedule learning rate to decay after loss somewhat plateaus.\n",
    "3. Use more modern optimization algorithms\n",
    "\n",
    "#### Testing/validating model\n",
    "Split data into 80-10-10 train-dev/validation-test splits. dev/validation split is the data used to search through hyperparameters. So we train several model on 80% of the data with different hyperparameters. Then we validate it on the dev split to see which hyperparameters turned out to be most optimal.\n",
    "\n",
    "Finally we test on the test split to see how our model has generalized. We can't do this too often as it risks overfitting on the test split. Not sure yet what to do if test split results are very poor (restart again? change parameters? ...)\n",
    "\n",
    "#### Exercises:\n",
    "1. Implement a search for \"optimal\" learning rate and an adaptive learning rate scheduler?\n",
    "2. Implement a few different optimization algorithms (Momentum, RMSprop, Adam, Adagrad)\n",
    "3. Test out different hyperparameters (hidden size, embedding size) using the validation-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3501d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "emb_dim = 2\n",
    "hid_dim = 100\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# Model parameters\n",
    "E = th.randn(VOCAB_SIZE, emb_dim, requires_grad=True)                    # embedding matrix\n",
    "\n",
    "W1 = th.randn((CONTEXT_WINDOW * emb_dim, hid_dim), requires_grad=True)   # first layer weights\n",
    "b1 = th.randn(hid_dim, requires_grad=True)                               # first layer bias\n",
    "\n",
    "W2 = th.randn((hid_dim, VOCAB_SIZE), requires_grad=True)                 # second layer weights\n",
    "b2 = th.randn(VOCAB_SIZE, requires_grad=True)                            # second layer bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438221f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.277381658554077\n",
      "Epoch 2, Loss: 2.552737236022949\n",
      "Epoch 3, Loss: 2.0968680381774902\n",
      "Epoch 4, Loss: 2.0689585208892822\n",
      "Epoch 5, Loss: 2.4757509231567383\n",
      "Epoch 6, Loss: 2.28945255279541\n",
      "Epoch 7, Loss: 2.310854911804199\n",
      "Epoch 8, Loss: 2.239020824432373\n",
      "Epoch 9, Loss: 3.0879671573638916\n",
      "Epoch 10, Loss: 2.375091552734375\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_X, batch_Y in train_dataloader:\n",
    "        # Forward pass\n",
    "        # We start with extracting the embedding for the batch\n",
    "        # emb = E[batch_X]                                        # shape: (batch_size, context_window, emb_dim)\n",
    "\n",
    "        # OPTIONAL: Use onehot encoding multiplication to index into embeddings\n",
    "        onehot = F.one_hot(batch_X, num_classes=VOCAB_SIZE).float()   # shape: (batch_size, context_window, vocab_size)\n",
    "        emb = onehot @ E                                              # shape: (batch_size, context_window,\n",
    "\n",
    "        # MLP forward pass\n",
    "        h = (emb.view(-1, CONTEXT_WINDOW * emb_dim) @ W1) + b1  # shape: (batch_size, hid_dim)\n",
    "        h_tanh = th.tanh(h)                                     # apply non-linearity\n",
    "        logits = (h_tanh @ W2) + b2                             # shape: (batch_size, VOCAB_SIZE)\n",
    "        \n",
    "        # Compute CE loss (manually) \n",
    "        # counts = logits.exp()\n",
    "        # Sum over first dimension because of batching. We have (batch_size, VOCAB_SIZE) where each row corresponds to\n",
    "        # the probabilities of a sample (up to batch size) over the entire vocabulary. This corresponds to dim=1 and we \n",
    "        # thus sum over dim=1. We keepdim=True because we want to maintain the (batch, VOCAB_SIZE) shape.\n",
    "        # probs = counts / counts.sum(1, keepdim=True)            # shape: (batch_size, VOCAB_SIZE)\n",
    "        # Extract the probabilities. Note that we have a batch of data, so we need to index accordingly\n",
    "        # For each sample in the batch, we want the probability of the true class (batch_Y). \n",
    "        # So from every row (corresponds to batch, which corresponds to the indices generated by th.arane(...)\n",
    "        # we extract its corresponding true class probability (indexed by batch_Y)\n",
    "        # loss = -probs[th.arange(batch_Y.shape[0]), batch_Y].log().mean()\n",
    "\n",
    "        # Alternatively we can use:\n",
    "        loss = F.cross_entropy(logits, batch_Y)\n",
    "\n",
    "        # Zero gradients from previous step\n",
    "        for param in [E, W1, b1, W2, b2]:\n",
    "            param.grad = None\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        with th.no_grad():\n",
    "            for param in [E, W1, b1, W2, b2]:\n",
    "                param += - LEARNING_RATE * param.grad\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "583f9ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dazeiah\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "context = [0] * CONTEXT_WINDOW    # initialize context with all '.'\n",
    "out_name = ''\n",
    "\n",
    "# E         := (|V|, emb_dim)\n",
    "# context   := (context_window,)\n",
    "while True:\n",
    "    ## Sample the next character\n",
    "    # First obtain embedding\n",
    "    # emb = E[th.tensor([context])]                                             # shape: (1, context_window, emb_dim)\n",
    "    onehot = F.one_hot(th.tensor([context]), num_classes=VOCAB_SIZE).float()    # shape: (1, context_window, emb_dim)\n",
    "    emb = onehot @ E\n",
    "\n",
    "    # Forward pass\n",
    "    h = (emb.view(-1, CONTEXT_WINDOW * emb_dim) @ W1) + b1  \n",
    "    h_tanh = th.tanh(h)                                     \n",
    "    logits = (h_tanh @ W2) + b2                             \n",
    "\n",
    "    # Sample from the distribution\n",
    "    counts = logits.exp() \n",
    "    probs = counts / counts.sum(1, keepdim=True)            # shape: (1, VOCAB_SIZE)\n",
    "    ix = th.multinomial(probs, num_samples=1).item()        # sample index\n",
    "    ch = itos[ix]                                           # get character from index\n",
    "\n",
    "    ## Add character to name\n",
    "    ch = itos[ix]  \n",
    "\n",
    "    # Stop if we reach end of name character\n",
    "    if ch == '.':\n",
    "        break\n",
    "    \n",
    "    out_name += ch\n",
    "\n",
    "    ## Update context window\n",
    "    # Update context by appending the current character and removing the starting char. This keeps the context array size fixed\n",
    "    context = context[1:] + [ix]       \n",
    "\n",
    "print(out_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9e4f0",
   "metadata": {},
   "source": [
    "# NN Using pytorch shortcuts\n",
    "\n",
    "Same model, same thing but now using all the pytorch functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6844dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V:    vocab size\n",
    "# m:    embedding size \n",
    "# h:    hidden size of hidden layer\n",
    "# n-1:  context window size\n",
    "\n",
    "class MLP(th.nn.Module):\n",
    "    def __init__(self, emb_dim, h):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim \n",
    "        self.h = h \n",
    "        \n",
    "        self.embedding = th.nn.Embedding(VOCAB_SIZE, emb_dim)       # embedding layer\n",
    "        self.fc1 = th.nn.Linear(CONTEXT_WINDOW * emb_dim, h)        # first fully connected layer\n",
    "        self.fc2 = th.nn.Linear(h, VOCAB_SIZE)                      # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        x:  (batch_size, n-1) tensor of input indices\n",
    "            n-1 because we concatenated n-1 inputs together to form the context\n",
    "\n",
    "        returns: (batch_size, V) tensor of output logits\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)                       # Returns (batch_size, context_window, emb_dim)\n",
    "        x = x.view(-1, CONTEXT_WINDOW * self.emb_dim)    # Returns (batch_size, context_window * emb_dim)\n",
    "        x = th.tanh(self.fc1(x))                    # Returns (batch_size, hid_dim)\n",
    "        x = self.fc2(x)                             # Returns (batch_size, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "46fa6129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n",
      "Epoch 1, Training Loss: 2.525839652014481\n",
      "Epoch 2, Training Loss: 2.377052959310724\n",
      "Epoch 3, Training Loss: 2.3201525572264177\n",
      "Epoch 4, Training Loss: 2.27967819517157\n",
      "Epoch 5, Training Loss: 2.2507068766241822\n",
      "Epoch 6, Training Loss: 2.2289081892993976\n",
      "Epoch 7, Training Loss: 2.211314871280257\n",
      "Epoch 8, Training Loss: 2.1965147044514874\n",
      "Epoch 9, Training Loss: 2.1842573696608967\n",
      "Epoch 10, Training Loss: 2.1734250657897847\n",
      "Epoch 11, Training Loss: 2.1638857355709664\n",
      "Epoch 12, Training Loss: 2.155269645896399\n",
      "Epoch 13, Training Loss: 2.147939968744468\n",
      "Epoch 14, Training Loss: 2.1403270829233847\n",
      "Epoch 15, Training Loss: 2.1339529671642254\n",
      "Epoch 16, Training Loss: 2.1279579317210264\n",
      "Epoch 17, Training Loss: 2.1226331071841935\n",
      "Epoch 18, Training Loss: 2.1173867183263306\n",
      "Epoch 19, Training Loss: 2.1125379034003235\n",
      "Epoch 20, Training Loss: 2.1081769354171205\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "model = MLP(emb_dim=10, h=200)  # n = context window + 1\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# print nr. of params\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "steps, losses = [], []\n",
    "# Train on train dataset\n",
    "for epoch in range(EPOCHS):  # number of epochs\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_Y in train_dataloader:\n",
    "        optimizer.zero_grad()                       # zero the gradients\n",
    "\n",
    "        # batch_X: (batch_size, context_window), contains the indices to extract\n",
    "        logits = model(batch_X)                     # forward pass\n",
    "        loss = F.cross_entropy(logits, batch_Y)     # compute loss\n",
    "        loss.backward()                             # backward pass\n",
    "        optimizer.step()                            # update parameters\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Store and print stats\n",
    "    steps.append(epoch)\n",
    "    losses.append(epoch_loss / len(train_dataloader))\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f5b96",
   "metadata": {},
   "source": [
    "#### Training and evaluating in practice\n",
    "How do we choose which model, hyperparameters, etc.. to use in practice?\n",
    "- Run models for various (sensible) choices of hyperparameters\n",
    "- Evaluate these models on the validation/dev split and choose the one that optimizes it\n",
    "- Pick out that model that optimizes dev split and evaluate it on test split **ONCE**\n",
    "- This evaluation is what we report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "63f21282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss curve\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(steps, losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "91e346da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3970410696754616\n",
      "Dev Loss: 2.3715264748724447\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "with th.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_Y in test_dataloader:\n",
    "        logits = model(batch_X)\n",
    "        loss = F.cross_entropy(logits, batch_Y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    print(f\"Test Loss: {avg_loss}\")\n",
    "\n",
    "# Dev loss \n",
    "with th.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_Y in dev_dataloader:\n",
    "        logits = model(batch_X)\n",
    "        loss = F.cross_entropy(logits, batch_Y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dev_dataloader)\n",
    "    print(f\"Dev Loss: {avg_loss}\")\n",
    "\n",
    "# If these losses seem similar then we are not overfitting. If we overfit then these losses would be much higher than the \n",
    "# training loss. This implies we could potentially increase the model capacity or train for longer without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a1730184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kille\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "# Generate new name by starting with context of all '.' and procedurally sampling the next character \n",
    "context = [0] * CONTEXT_WINDOW    # initialize context with all '.'\n",
    "out_name = ''\n",
    "\n",
    "while True:\n",
    "    # Sample the next character\n",
    "    out = model(th.tensor([context]))\n",
    "    probs = F.softmax(out, dim=1)  # get probabilities\n",
    "    ix = th.multinomial(probs, num_samples=1).item()  # sample index\n",
    "    ch = itos[ix]                                     # get character from index\n",
    "    \n",
    "    if ch == '.':\n",
    "        break\n",
    "    out_name += ch\n",
    "\n",
    "    # Update context window\n",
    "    context = context[1:] + [ix]   \n",
    "    \n",
    "print(out_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-study-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
